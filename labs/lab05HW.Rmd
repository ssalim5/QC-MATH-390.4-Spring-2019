---
title: "Lab 5"
author: "Sakib Salim"
output: pdf_document
date: "11:59PM March 17, 2019"
---

Load the Boston housing data frame and create the vector $y$ (the median value) and matrix $X$ (all other features) from the data frame. Name the columns the same as Boston except for the first name it "(Intercept)".

```{r}
data(Boston, package = "MASS")
y = Boston$medv
X = as.matrix(cbind(1, Boston[, 1 : 13]))
colnames(X)[1] = "(Intercept)"
```

Run the OLS linear model to get $b$, the vector of coefficients. Do not use `lm`.

```{r}
b = solve(t(X) %*% X) %*% t(X) %*% y
```

Find the hat matrix for this regression `H` and find its rank. Is this rank expected?

```{r}
H = X %*% solve(t(X) %*% X) %*% t(X)
pacman::p_load(Matrix)
rankMatrix(H)
```

Verify this is a projection matrix by verifying the two sufficient conditions. Use the `testthat` library's `expect_equal(matrix1, matrix2, tolerance = 1e-2)`.

```{r}
pacman::p_load(testthat)
expect_equal(H, t(H), tolerance = 1e-2)
expect_equal(H %*% H, H, tolerance = 1e-2)
```

Find the matrix that projects onto the space of residuals `H_comp` and find its rank. Is this rank expected?

```{r}
I = diag(nrow(H))
H_comp = (I - H)
rankMatrix(H_comp)
```

Verify this is a projection matrix by verifying the two sufficient conditions. Use the `testthat` library.

```{r}
expect_equal(H_comp, t(H_comp), tolerance = 1e-2)
expect_equal(H_comp %*% H_comp, H_comp, tolerance = 1e-2)
```

Calculate $\hat{y}$.

```{r}
yhat = H %*% y
head(yhat)
```

Calculate $e$ as the difference of $y$ and $\hat{y}$ and the projection onto the space of the residuals. Verify the two means of calculating the residuals provide the same results.

```{r}
e = y - yhat
e_2 = H_comp %*% y
expect_equal(e, e_2)
```

Calculate $R^2$ and RMSE.

```{r}
sse = sum(e^2)
sst = sum((y - mean(y))^2)

Rsquared = 1 - sse / sst
Rsquared

mse = sse / (nrow(X) - ncol(X))
rmse = sqrt(mse) #rmse is standard deviation of errors
rmse
```

Verify $\hat{y}$ and $e$ are orthogonal.

```{r}
t(e) %*% yhat
```

Verify $\hat{y} - \bar{y}$ and $e$ are orthogonal.

```{r}
t(e) %*% (yhat - mean(y))
```

Find the cosine-squared of $y - \bar{y}$ and $\hat{y} - \bar{y}$ and verify it is the same as $R^2$.

```{r}
y_minus_y_bar = y - mean(y)
yhat_minus_y_bar = yhat - mean(y)
len_y_minus_y_bar = sqrt(  sum(y_minus_y_bar^2)  )
len_yhat_minus_y_bar = sqrt(  sum(yhat_minus_y_bar^2)  )

theta = acos( (t(y_minus_y_bar) %*% yhat_minus_y_bar) / (len_y_minus_y_bar * len_yhat_minus_y_bar) )
cos_theta_sqrd = cos(theta)^2
cos_theta_sqrd
Rsquared
```

Verify the sum of squares identity which we learned was due to the Pythagorean Theorem (applies since the projection is specifically orthogonal).

```{r}
len_y_minus_y_bar^2 - len_yhat_minus_y_bar^2 - sse
```

Create a matrix that is $(p + 1) \times (p + 1)$ full of NA's. Label the columns the same columns as X. Do not label the rows. For the first row, find the OLS estimate of the $y$ regressed on the first column only and put that in the first entry. For the second row, find the OLS estimates of the $y$ regressed on the first and second columns of $X$ only and put them in the first and second entries. For the third row, find the OLS estimates of the $y$ regressed on the first, second and third columns of $X$ only and put them in the first, second and third entries, etc. For the last row, fill it with the full OLS estimates.

```{r}
M = matrix(NA, nrow = ncol(X), ncol = ncol(X))
colnames(M) = colnames(X)
X_j = X[, 1, drop = FALSE]
b = solve(t(X_j) %*% X_j) %*% t(X_j) %*% y
M[1, 1] = b
X_j_2 = X[ , 1:2]
b = solve(t(X_j_2) %*% X_j_2) %*% t(X_j_2) %*% y
b
for(j in 1 : ncol(M)){
  X_j = X[, 1 : j, drop = FALSE]
  b = solve(t(X_j) %*% X_j) %*% t(X_j) %*% y
  M[j, 1:j] = b
}
round(M, 2)
```

Examine this matrix. Why are the estimates changing from row to row as you add in more predictors?

#TO-DO

Clear the workspace and load the diamonds dataset.

```{r}
rm(list = ls())
pacman::p_load(ggplot2)
data(diamonds, package = "ggplot2")
```

Extract $y$, the price variable and "c", the nominal variable "color" as vectors.

```{r}
summary(diamonds)
y = diamonds$price
c = diamonds$color
table(c)
```

Convert the "c" vector to $X$ which contains an intercept and an appropriate number of dummies. Let the color G be the refernce category as it is the modal color. Name the columns of $X$ appropriately. The first should be "(Intercept)". Delete c.

```{r}
X = rep(1, nrow(diamonds))
X = cbind(X, diamonds$color == 'D')
X = cbind(X, diamonds$color == 'E')
X = cbind(X, diamonds$color == 'F')
X = cbind(X, diamonds$color == 'H')
X = cbind(X, diamonds$color == 'I')
X = cbind(X, diamonds$color == 'J')
colnames(X) = c("Intercept", "is_D", "is_E", "is_F", "is_H", "is_I", "is_J")
head(X)
```

Repeat the iterative exercise above we did for Boston here.

```{r}
#TO-DO
```

Why didn't the estimates change as we added more and more features?

#TO-DO

Create a vector $y$ by simulating $n = 100$ standard iid normals. Create a matrix of size 100 x 2 and populate the first column by all ones (for the intercept) and the second column by 100 standard iid normals. Find the $R^2$ of an OLS regression of `y ~ X`. Use matrix algebra.

```{r}
n = 100
p = 2
y = rnorm(n)
X = rep(1,n)
X = cbind(X,rnorm(n))
H = X %*% solve(t(X) %*% X) %*% t(X)
yhat = H %*% y
e = y-yhat
ybar = mean(y)
dev_mean = y-ybar
SSE = sum(e^2)
SST = sum(dev_mean^2)
Rsqd = 1-SSE/SST
Rsqd
summary(lm(y~X))$r.squared
```

 from the last problem. Find the $R^2$ of an OLS regression of `y ~ X`. You can use the `summary` function of an `lm` model.

Write a for loop to each time bind a new column of 100 standard iid normals to the matrix $X$ and find the $R^2$ each time until the number of columns is 100. Create a vector to save all $R^2$'s. What happened??


```{r}
#TO-DO
rsqd_vec = rep(NA,n-2)
for ( i in 1:(n-2)){
  new_col = rnorm(n)
  X = cbind(X, new_col)
  rsqd_vec[i] = summary(lm(y~X))$r.squared
}
rsqd_vec
#R^2 = 1 when the number of columns (i.e. features) equals the number of observations
```

Add one final column to $X$ to bring the number of columns to 101. Then try to compute $R^2$. What happens and why?

```{r}
#TO-DO
X = cbind(X,rnorm(n))
summary(lm(y~X))$r.squared
#The supremum of R^2 is 1 and there is no way it could be greater than 1 since SSE/SST > 0.  
```

